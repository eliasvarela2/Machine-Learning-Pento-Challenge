{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b5f5c2",
   "metadata": {},
   "source": [
    "### Clarification about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e6471",
   "metadata": {},
   "source": [
    "#### I'd like to explain that I collected some more images of the four breeds that we want to predict. And I used the images provided by Pento to check, at the end, if the model predicts them correctly. I also used a picture of a Dobermann, to see if the predicted class was 'other'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6aefaf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334693fd",
   "metadata": {},
   "source": [
    "## We expand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191d777",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path of the file\n",
    "tar_file_path = './stanford_dogs/images.tar'\n",
    "\n",
    "# Folder where the files will be extracted\n",
    "extract_path = './stanford_dogs/'\n",
    "\n",
    "# Extract the file\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "    print(f\"Files extracted in: {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd452b63",
   "metadata": {},
   "source": [
    "## Load the data obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713de36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the route to the main folder that contains the classes\n",
    "base_dir = r'.\\stanford_dogs\\images\\filtered_images'\n",
    "\n",
    "# Create an empty list to save the images and the labels\n",
    "img_dataset = []\n",
    "labels = []\n",
    "\n",
    "# Classes that we need\n",
    "class_mapping = {\n",
    "    'n02099601-golden_retriever': 'Golden Retriever',\n",
    "    'n02106662-German_shepherd': 'German Shepherd',\n",
    "    'n02108915-French_bulldog': 'French Bulldog',\n",
    "    'poodle': 'Poodle'\n",
    "}\n",
    "\n",
    "# Run through every class folder\n",
    "for class_name in os.listdir(base_dir):\n",
    "    class_dir = os.path.join(base_dir, class_name)\n",
    "    \n",
    "    # Make sure that is a directory\n",
    "    if os.path.isdir(class_dir):\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        # Obtaine the grouped label\n",
    "        label = class_mapping.get(class_name)\n",
    "        \n",
    "        # Run every image file in the subfolder\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            \n",
    "            try:\n",
    "                # Load and conver the image in a numpy array\n",
    "                img = load_img(img_path, target_size=(299, 299))  # Re-dimension to 299x299\n",
    "                img_array = img_to_array(img)\n",
    "                \n",
    "                # Add the image to the dataset\n",
    "                img_dataset.append(img_array)\n",
    "                labels.append(label)  # Save the corresponding label\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error when processing image {img_file}: {e}\")\n",
    "\n",
    "# Conver the list of images to a numpy array\n",
    "if img_dataset:\n",
    "    img_dataset = np.array(img_dataset)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"Dataset of images: {img_dataset.shape}\")\n",
    "    print(f\"Labels: {labels.shape}\")\n",
    "else:\n",
    "    print(\"Images were not found in the specified folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d577c",
   "metadata": {},
   "source": [
    "## Load data, preprocess, training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the route to the main folder that contains all the classes\n",
    "base_dir = r'./stanford_dogs/images/filtered_images'\n",
    "\n",
    "# Create a datagenerator with preprocess_input for InceptionV3\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  \n",
    "    validation_split=0.15,  \n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    brightness_range=[0.7, 1.3]\n",
    ")\n",
    "\n",
    "# Training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(299, 299),  # Tamaño requerido por InceptionV3\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bbf1b",
   "metadata": {},
   "source": [
    "## Load a pre-trained model, add new personalized layers, create final model and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inceptionv3 model withouth the top layers (include_top = False)\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add new personalized layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  \n",
    "x = Dropout(0.5)(x) \n",
    "x = Dense(1024, activation='relu')(x)  \n",
    "x = Dropout(0.5)(x) \n",
    "predictions = Dense(4, activation='softmax')(x)  \n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the InceptionV3 layers so we don't train them\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with Adam, and the metrics: accuracy and precision\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b602a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb986ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save the history\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc, val_prec = model.evaluate(validation_generator)\n",
    "print(f\"Accuracy in validation set: {val_acc * 100:.2f}%\")\n",
    "print(f\"Precision in validation set: {val_prec * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd14877",
   "metadata": {},
   "source": [
    "## Save the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the specified folder\n",
    "\n",
    "model.save(r'./pento-ssr-challenge/my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e953547",
   "metadata": {},
   "source": [
    "## Load the saved model from the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed060782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the specified folder\n",
    "model = load_model(r'./pento-ssr-challenge/my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3e05c",
   "metadata": {},
   "source": [
    "## Plot the accuracy / precision vs epoch and Loss Vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e832b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    # Obtener métricas\n",
    "    loss = history.history.get('loss', [])\n",
    "    val_loss = history.history.get('val_loss', [])\n",
    "    accuracy = history.history.get('accuracy', [])\n",
    "    val_accuracy = history.history.get('val_accuracy', [])\n",
    "    precision = history.history.get('precision_2', []) \n",
    "    val_precision = history.history.get('val_precision_2', [])  \n",
    "    \n",
    "    # Create a figure with sub-graphics\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Loss chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss, label='Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Precission chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracy, label='Accuracy')\n",
    "    plt.plot(val_accuracy, label='Val Accuracy')\n",
    "    plt.plot(precision, label='Precision')\n",
    "    plt.plot(val_precision, label='Val Precission')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy / Precission')\n",
    "    plt.title('Accuracy / Precission vs Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the history of training\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82577f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "image_path = r'./pento-ssr-challenge/metrics.png'\n",
    "\n",
    "# Load the image\n",
    "img = mpimg.imread(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Optional: Hide the axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112498a",
   "metadata": {},
   "source": [
    "### Here, we define a classify_images function. It sets a probability threshold to classificate if the image of the dog belongs to one of our classes or if it should belong to 'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(image_dir):\n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model('./pento-ssr-challenge/my_model.keras')\n",
    "\n",
    "    # Define class labels\n",
    "    class_labels = {0: 'golden_retriever', 1: 'german_shepherd', 2: 'french_bulldog', 3: 'poodle', 4: 'toy_poodle', 5: 'standard_poodle'}\n",
    "    \n",
    "    # Define the probability threshold\n",
    "    threshold = 0.75\n",
    "\n",
    "    # Recursively iterate over each file in the directory and subdirectories\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for img_file in files:\n",
    "            img_path = os.path.join(root, img_file)\n",
    "\n",
    "            # Ensure it's a file\n",
    "            if os.path.isfile(img_path):\n",
    "                try:\n",
    "                    # Load and preprocess the image\n",
    "                    img = load_img(img_path, target_size=(299, 299))\n",
    "                    img_array = img_to_array(img)\n",
    "                    img_array = np.expand_dims(img_array, axis=0)\n",
    "                    img_array = preprocess_input(img_array)\n",
    "\n",
    "                    # Make the prediction\n",
    "                    predictions = model.predict(img_array)\n",
    "                    max_prob = np.max(predictions)\n",
    "                    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "                    # Get the corresponding label\n",
    "                    if max_prob >= threshold:\n",
    "                        class_label = class_labels.get(predicted_class, 'other')\n",
    "                    else:\n",
    "                        class_label = 'other'\n",
    "\n",
    "                    print(f\"Image: {img_file}, Classification: {class_label} (Probability: {max_prob:.2f})\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_file}: {e}\")\n",
    "\n",
    "# Path to the directory with images\n",
    "new_images_dir = 'C:/Users/Julian Amuedo/Desktop/pento-ssr-challenge/dogs'\n",
    "\n",
    "# Call the function\n",
    "classify_images(new_images_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d311e",
   "metadata": {},
   "source": [
    "### Code breakdown for the image data generator and it's correspondant generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the route to the main folder that contains all the classes\n",
    "base_dir = r'./stanford_dogs/images/filtered_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77f98e",
   "metadata": {},
   "source": [
    "#### This is the path to the directory where the images are organized into subdirectories, each corresponding to a different class. Each subdirectory name should match a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datagenerator with preprocess_input for InceptionV3\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  \n",
    "    validation_split=0.15,  \n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    brightness_range=[0.7, 1.3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47dfc4",
   "metadata": {},
   "source": [
    "### ImageDataGenerator: A class from TensorFlow/Keras used to generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "### preprocessing_function=preprocess_input: Applies preprocessing specific to the InceptionV3 model. This step is essential to scale and normalize the images as required by the pre-trained model.\n",
    "\n",
    "### validation_split=0.15: Specifies that 15% of the images will be used for validation. This means the remaining 85% will be used for training. The ImageDataGenerator will use this split to automatically create validation data from the training data.\n",
    "\n",
    "### horizontal_flip=True: Randomly flips the images horizontally. This augmentation technique helps the model generalize better by seeing the same image in different orientations.\n",
    "\n",
    "### rotation_range=30: Randomly rotates the images within a range of 30 degrees. This helps the model become invariant to slight rotations.\n",
    "\n",
    "### width_shift_range=0.2: Randomly shifts the images horizontally by a fraction of the width (20% of the width). This makes the model more robust to slight changes in the image's horizontal position.\n",
    "\n",
    "### height_shift_range=0.2: Randomly shifts the images vertically by a fraction of the height (20% of the height). Similar to width_shift_range, this helps with vertical positional changes.\n",
    "\n",
    "### zoom_range=0.2: Randomly zooms into the images by 20%. This helps the model handle variations in the scale of objects.\n",
    "\n",
    "### shear_range=0.2: Applies random shearing transformations to the images. Shearing is a form of distortion where the image is stretched in one direction. This can improve the model’s robustness to such distortions.\n",
    "\n",
    "### brightness_range=[0.7, 1.3]: Randomly changes the brightness of the images within the specified range. This helps the model handle variations in lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(299, 299),  # Tamaño requerido por InceptionV3\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ca90b",
   "metadata": {},
   "source": [
    "### train_generator: This is the generator that will provide batches of images for training.\n",
    "\n",
    "### base_dir: The directory containing the images organized in subdirectories by class.\n",
    "\n",
    "### target_size=(299, 299): Resizes all images to 299x299 pixels, which is the input size expected by the InceptionV3 model.\n",
    "\n",
    "### batch_size=32: Number of images to return in each batch.\n",
    "\n",
    "### class_mode='categorical': Specifies that the labels are one-hot encoded vectors (for multi-class classification).\n",
    "\n",
    "### subset='training': Indicates that this generator should use the portion of the data designated for training, as specified by the validation_split parameter.\n",
    "\n",
    "### validation_generator: This generator provides batches of images for validation.\n",
    "\n",
    "### subset='validation': Indicates that this generator should use the portion of the data designated for validation.    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79709b",
   "metadata": {},
   "source": [
    "# Model explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa83f68",
   "metadata": {},
   "source": [
    "### Model Explanation\n",
    "\n",
    "1. **Base Model: InceptionV3 (without top layers)**\n",
    "\n",
    "   We load the pre-trained InceptionV3 model without the top layers (i.e., without the fully connected layers) to use it as a feature extractor.\n",
    "\n",
    "   $$ f(\\mathbf{X}) = \\text{InceptionV3}(\\mathbf{X}, \\text{weights} = \\text{imagenet}, \\text{include\\_top} = \\text{False}) $$\n",
    "\n",
    "   where $$ f(\\mathbf{X}) $$ is the input image.\n",
    "\n",
    "2. **New Personalized Layers**\n",
    "\n",
    "   - **Global Average Pooling Layer:**\n",
    "\n",
    "     The global average pooling layer computes the average of each feature map across its spatial dimensions:\n",
    "\n",
    "     $$ \\mathbf{z} = \\text{GlobalAveragePooling2D}(f(\\mathbf{X})) $$\n",
    "\n",
    "   - **Dropout Layer (0.5):**\n",
    "\n",
    "     Dropout randomly sets a fraction \\( p = 0.5 \\) of the input units to zero during training:\n",
    "\n",
    "     $$ \\mathbf{z}' = \\text{Dropout}(0.5) \\mathbf{z} $$\n",
    "\n",
    "   - **Dense Layer (1024 units, ReLU activation):**\n",
    "\n",
    "     A fully connected dense layer with 1024 units and ReLU activation:\n",
    "\n",
    "     $$ \\mathbf{h} = \\text{ReLU}(W \\mathbf{z}' + b) $$\n",
    "\n",
    "     where \\( W \\) is the weight matrix, \\( b \\) is the bias vector, and $$ \\mathbf{h} $$ is the output.\n",
    "\n",
    "   - **Dropout Layer (0.5):**\n",
    "\n",
    "     Another dropout layer is applied:\n",
    "\n",
    "     $$ \\mathbf{h}' = \\text{Dropout}(0.5)(\\mathbf{h}) $$\n",
    "\n",
    "   - **Output Dense Layer (4 units, Softmax activation):**\n",
    "\n",
    "     The final dense layer with 4 units (for the 4 classes) and softmax activation to get class probabilities:\n",
    "\n",
    "     $$ \\mathbf{y} = \\text{Softmax}(W' \\mathbf{h}' + b') $$\n",
    "\n",
    "3. **Model Compilation**\n",
    "\n",
    "   - **Adam Optimizer:**\n",
    "\n",
    "     The Adam optimizer is defined as:\n",
    "\n",
    "     $$ \\text{AdamOptimizer}(\\text{learning\\_rate} = 0.0001) $$\n",
    "\n",
    "   - **Categorical Cross-Entropy Loss:**\n",
    "\n",
    "     The loss function is categorical cross-entropy:\n",
    "\n",
    "     $$ \\text{Loss} = - \\sum_{i=1}^{C} y_i \\log(p_i) $$\n",
    "\n",
    "     where \\( C \\) is the number of classes, \\( y_i \\) is the true label, and \\( p_i \\) is the predicted probability for class \\( i \\).\n",
    "\n",
    "   - **Accuracy Metric:**\n",
    "\n",
    "     Accuracy measures the proportion of correctly classified samples:\n",
    "\n",
    "     $$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
    "\n",
    "   - **Precision Metric:**\n",
    "\n",
    "     Precision calculates the ratio of true positive predictions to the total number of positive predictions:\n",
    "\n",
    "     $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "     where \\( TP \\) is the number of true positives and \\( FP \\) is the number of false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de853b",
   "metadata": {},
   "source": [
    "## Extra comments i'd like to do in person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7344454",
   "metadata": {},
   "source": [
    "Why I chose InceptionV3: I could explain why I selected InceptionV3 as my base model, emphasizing its success in image classification tasks and how its pre-trained weights on ImageNet speed up training and improve performance.\n",
    "\n",
    "Freezing layers: I'd discuss the importance of freezing the layers of the pre-trained model, explaining that it prevents updating the weights during training, which preserves the learned features and allows the new layers to focus on learning high-level representations for my specific task.\n",
    "\n",
    "Custom layers: I would explain the reasoning behind adding a GlobalAveragePooling2D layer, a common practice to reduce overfitting while retaining spatial information, and how the additional dense and dropout layers help in reducing overfitting by adding regularization.\n",
    "\n",
    "Softmax activation: I'd clarify why I used a softmax layer with 4 outputs, corresponding to the 4 dog breeds I’m trying to classify, ensuring that the model outputs class probabilities that sum to 1.\n",
    "\n",
    "Why precision as a metric: I’d go into why I chose to track precision, especially in cases where misclassifying one breed for another (e.g., a similar-looking breed) might be more harmful and precision helps capture this.\n",
    "\n",
    "Regularization with dropout: I could explain how dropout reduces overfitting by randomly ignoring a fraction of the neurons during training, which forces the network to generalize better.\n",
    "\n",
    "How we could benefit from more data (images) of other breeds: I would like to explain that, providing more breeds could lead to  a better prediction of the class (label) 'others'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d8b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
